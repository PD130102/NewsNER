{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = \"वैज्ञानिकों जॉर्जिया मिला तस्दीक 8,000 साल पुराने मिट्टी मर्तबान उसमें अंगूर शराब बनाए सबूत शोधकर्ताओं राय अंगूर शराब बनाने सबसे शुरुआती प्रमाण जॉर्जिया राजधानी तबलिसी दक्षिणी इलाकों जगहों मिले हैं.ये जगहें नवपाषाण युग संबंधित शराब अवशेष मिले मर्तबानों अंगूर गुच्छे नाचते शख़्स तस्वीर है.जॉर्जिया मिली चीज़ों बारे प्रोसीडिंग्स ऑफ़ नेशनल एकैडमी ऑफ़ साइसेंज रिसर्च रिपोर्ट छपी है.शराबी पतलून निकला अजगर ... शराब पीने लोग अंग्रेज़ी बोलते टोरंटो यूनिवर्सिटी सीनियर रिसर्चर रिसर्च सह-लेखक स्टीफ़न बाट्युक मानना जंगल उगने यूरेशियाई अंगूरों शराब बनाने सबसे पुराना उदाहरण स्टीफ़न राय जानते पश्चिम सभ्यताओं शराब ख़ास दवा तौर सामाजिक मेलजोल बहलाने शराब समाज अर्थव्यवस्था दवाओं खान-पान केंद्र इससे शराब बनाने सबसे पुराने प्रमाण मिले ईरान पाए ईरान मिले शराब मर्तबानों उम्र सात हज़ार साल बताई थी.साल अर्मेनिया गुफा हज़ार पुरानी शराब अवशेष मिले दुनिया सबसे पुरानी बिना अंगूर वाली शराब बारे सात हज़ार साल पुरानी चीन मिली शराब चावल शहद फलों बीबीसी हिन्दी एंड्रॉएड क्लिक हमें फ़ेसबुक ट्विटर फ़ॉलो\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/Users/pranavdeepak/miniforge3/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"csebuetnlp/mT5_m2o_hindi_crossSum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\n",
    "    [WHITESPACE_HANDLER(article_text)],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")[\"input_ids\"]\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=84,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_beams=4\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "शोधकर्ताओं का कहना है कि उन्हें इसराइल में हैफ़ा के नज़दीक प्रागैतिहासिक काल की एक गुफा से दुनिया की सबसे पुरानी शराब की भट्टी मिली है. साथ ही 13,000 साल बाद पीने वाली चीज़ें भी इसकी चपेट में हैं.\n"
     ]
    }
   ],
   "source": [
    "summary = tokenizer.decode(\n",
    "    output_ids,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "# pickle th model \n",
    "with open('./models/TS_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# pickle the tokenizer\n",
    "with open('./models/TS_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the above code as function which take in a article text and return summary\n",
    "# Load the model and tokenizer from pickle file and use it to generate summary\n",
    "# Path: TS.py\n",
    "import re\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "\n",
    "model3 = pickle.load(open(\"./models/TS_model.pkl\", \"rb\"))\n",
    "tokenizer3 = pickle.load(open(\"./models/TS_tokenizer.pkl\", \"rb\"))\n",
    "\n",
    "def text_summarizer(article_text):\n",
    "\n",
    "    input_ids = tokenizer3(\n",
    "        [WHITESPACE_HANDLER(article_text)],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    output_ids = model3.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=84,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_beams=4\n",
    "    )[0]\n",
    "\n",
    "    summary = tokenizer3.decode(\n",
    "        output_ids,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'शोधकर्ताओं का कहना है कि उन्हें इसराइल में हैफ़ा के नज़दीक प्रागैतिहासिक काल की एक गुफा से दुनिया की सबसे पुरानी शराब की भट्टी मिली है. साथ ही 13,000 साल बाद पीने वाली चीज़ें भी इसकी चपेट में हैं.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summarizer(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "import pandas as pd\n",
    "# Load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"csebuetnlp/mT5_m2o_hindi_crossSum\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"csebuetnlp/mT5_m2o_hindi_crossSum\")\n",
    "\n",
    "# Prepare your dataset and DataLoader (not shown here)\n",
    "\n",
    "data = pd.read_csv(\"./dataset/preprocessed_text.csv\")\n",
    "data = data.dropna()\n",
    "data = data.reset_index(drop=True)\n",
    "data = data.iloc[:1000]\n",
    "data = data[data['text'].str.len() < 512]\n",
    "data = data[data['text'].str.len() > 100]\n",
    "data = data.reset_index(drop=True)\n",
    "# Add the summarization column\n",
    "data['summary'] = None\n",
    "\n",
    "# Define a custom Dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, data):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        text = row['text']\n",
    "        summary = row['summary']\n",
    "\n",
    "        source_encoding = tokenizer(\n",
    "            [WHITESPACE_HANDLER(text)],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "        target_encoding = tokenizer(\n",
    "            [WHITESPACE_HANDLER(summary)],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=150\n",
    "        )\n",
    "\n",
    "        labels = target_encoding[\"input_ids\"]\n",
    "        labels[labels[:, :] == 0] = -100\n",
    "\n",
    "        return {\n",
    "            \"source_ids\": source_encoding[\"input_ids\"].flatten(),\n",
    "            \"source_mask\": source_encoding[\"attention_mask\"].flatten(),\n",
    "            \"target_ids\": target_encoding[\"input_ids\"].flatten(),\n",
    "            \"target_mask\": target_encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": labels.flatten()\n",
    "        }\n",
    "    \n",
    "# Create a DataLoader for training and validation\n",
    "train_dataset = CustomDataset(tokenizer, data)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define training parameters\n",
    "learning_rate = 1e-5\n",
    "epochs = 3\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Start training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1} of {epochs}\")\n",
    "    model.train()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    epoch_loss = 0\n",
    "    for batch, data in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        source_ids = data['source_ids'].to(device, dtype=torch.long)\n",
    "        source_mask = data['source_mask'].to(device, dtype=torch.long)\n",
    "        target_ids = data['target_ids'].to(device, dtype=torch.long)\n",
    "        target_mask = data['target_mask'].to(device, dtype=torch.long)\n",
    "        labels = data['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=source_ids,\n",
    "            attention_mask=source_mask,\n",
    "            labels=labels,\n",
    "            decoder_attention_mask=target_mask,\n",
    "            use_cache=False\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Iteration {batch} of {len(train_dataloader)} loss: {loss.item()}\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} average loss: {epoch_loss / len(train_dataloader)}\")\n",
    "    torch.save(model.state_dict(), f\"t5_summary_{epoch + 1}.pth\")\n",
    "\n",
    "# Save trained model\n",
    "model.save_pretrained(\"t5_summary\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
